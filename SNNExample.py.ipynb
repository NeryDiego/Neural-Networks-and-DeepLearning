{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()  \n",
    "#extracting first 100 samples pertaining #to iris setosa and verginica  \n",
    "X = iris.data[:100, :4]  \n",
    "#actual output  \n",
    "Y = iris.target[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Dimensions Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "# Now the data X_norm is of shape (100, 4)\n",
    "# But for SNN we need to have the data of shape (no_of_features x no_of_samples). So take a transpose of X_norm\n",
    "# The data Y is a list of shape (100,). Convert it to a vector of shape (1, 100) by using reshape() function\n",
    "X_norm = X.reshape(100,4)\n",
    "X_data = X_norm.T\n",
    "Y_data = Y.reshape(1,100)\n",
    "\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Weights and Bias\n",
    "\n",
    "* Before we start forward propagation, we need to initialize weights and bias to some random values.\n",
    "\n",
    "* Since we have four features(length and the width of the sepals and petals, in centimeters), we need to have weight vector of shape (4,1) and one bias term of shape (1,1)\n",
    "\n",
    "* In this case, we initialize all our weights and bias to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseNetwork(num_features):\n",
    "    W = np.zeros((num_features, 1))\n",
    "    b = 0\n",
    "    parameters = {\"W\": W, \"b\":b}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Activation Function\n",
    "\n",
    "* Before going with the forward propagation, we need to define an activation function for the neuron.\n",
    "\n",
    "* Since this is a binary classification, let's consider a sigmoid function that maps any linear input to values between 0 to 1\n",
    "\n",
    "* The sigmoid activation function is implemented as shown in the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, Y, parameters):\n",
    "    W = parameters[\"W\"]\n",
    "    \n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Z = np.dot(W.T,X) + b\n",
    "    \n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating cost function for a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A, Y, num_samples):\n",
    "    return -1/num_samples *np.sum(Y*np.log(A) + (1-Y)*(np.log(1-A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Backpropagation\n",
    "\n",
    "* From forward propagation, you know the output A\n",
    "\n",
    "* Using this output, you need to find the derivatives of weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(X, Y, A, num_samples):\n",
    "    dZ = A - Y\n",
    "    \n",
    "    dW = (np.dot(X,dZ.T))/num_samples\n",
    "    \n",
    "    db = np.sum(dZ)/num_samples\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Parameters\n",
    "\n",
    "* Once we have the derivatives, you need to subtract them from original weights and bias\n",
    "\n",
    "* While subtracting, we multiply the derivatives with a learning rate to have control over the gradient at each step of iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, dW, db, learning_rate):\n",
    "    W = parameters[\"W\"] - (learning_rate * dW)\n",
    "    \n",
    "    b = parameters[\"b\"] - (learning_rate * db)\n",
    "    \n",
    "    return {\"W\": W, \"b\": b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "* Using all the function defined so far let's define the model to initialize and train the SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, num_iter, learning_rate):\n",
    "    num_features = X.shape[0]\n",
    "    num_samples = float(X.shape[1])\n",
    "    parameters = initialiseNetwork(num_features)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        A = forwardPropagation(X, Y, parameters)\n",
    "        if(i % 100 == 0):\n",
    "            print(\"Cost after {} iteration: {}\".format(i, cost(A, Y, num_samples)))\n",
    "        dW, db = backPropagation(X, Y, A, num_samples)\n",
    "        parameters = updateParameters(parameters, dW, db, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "* Train the model using iris dataset with learning rate 0.1 and number of iteration equal to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0 iteration: 0.6931471805599453\n",
      "Cost after 100 iteration: 0.0665609597638373\n",
      "Cost after 200 iteration: 0.03492693805012101\n",
      "Cost after 300 iteration: 0.02393176289715331\n",
      "Cost after 400 iteration: 0.018316011387467378\n",
      "Cost after 500 iteration: 0.01489436577723384\n",
      "Cost after 600 iteration: 0.012584644199696765\n",
      "Cost after 700 iteration: 0.01091714770363378\n",
      "Cost after 800 iteration: 0.009654708260872398\n",
      "Cost after 900 iteration: 0.008664477279333902\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_data, Y, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can see that at every iteration the cost is reducing approaching close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
